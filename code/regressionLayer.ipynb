{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4849e1e-1928-4e8b-a12f-37c786b50aca",
   "metadata": {},
   "source": [
    "# Regression Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dddcb2a8-73d3-4476-b88a-bc1494a2c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/researchsrv1/anaconda3/envs/des4clem/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, PowerTransformer\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import friedmanchisquare, rankdata\n",
    "import shap\n",
    "import scikit_posthocs as sp\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from IPython.display import FileLink, display\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt.space import Integer, Real\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn import tree\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.callbacks import VerboseCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, VotingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from lightgbm import LGBMRegressor\n",
    "from IPython.display import display, FileLink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6683c-fd3f-4daa-afdc-2e7f83c3fce3",
   "metadata": {},
   "source": [
    "### Preparation before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec17e47a-8d92-498d-8f28-d8259d6ebc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2763 entries, 0 to 2762\n",
      "Columns: 318 entries, GENDER to total_sum\n",
      "dtypes: float64(47), int64(2), object(269)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Call Dataset\n",
    "pd.set_option('display.max_rows', 10)\n",
    "initial_df = pd.read_csv('3labelv4Regression.csv')\n",
    "initial_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3751352-73bc-42c6-b6a5-84a3badf14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All categorical features except for label\n",
    "cols = initial_df.columns\n",
    "num_cols = initial_df._get_numeric_data().columns\n",
    "categorical_features = list(set(cols) - set(num_cols))\n",
    "categorical_features.remove('depression_category')\n",
    "\n",
    "# Label Encode all categorical, but keep missing values\n",
    "le_initial_df = initial_df.copy()\n",
    "dropped_labels = le_initial_df['depression_category']\n",
    "le_initial_df = le_initial_df.drop('depression_category', axis = 1)\n",
    "\n",
    "for col in le_initial_df.columns:\n",
    "    if le_initial_df[col].dtype == 'object':\n",
    "        le_initial_df[col] = le_initial_df[col].fillna('missing')\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        le_initial_df[col] = label_encoder.fit_transform(le_initial_df[col])\n",
    "\n",
    "        missing_value_index = np.where(label_encoder.classes_ == 'missing')[0]\n",
    "        \n",
    "        le_initial_df[col] = le_initial_df[col].replace(missing_value_index, np.nan)\n",
    "\n",
    "le_initial_df = pd.concat([le_initial_df, dropped_labels], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa21a95-21f1-4274-86ba-d7977813066b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGEGRP</th>\n",
       "      <th>DEGREE_RECODE</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>RACE_RECODE</th>\n",
       "      <th>HISPANIC</th>\n",
       "      <th>ETHGRP</th>\n",
       "      <th>MILITARY</th>\n",
       "      <th>JAIL</th>\n",
       "      <th>...</th>\n",
       "      <th>IWLOC5</th>\n",
       "      <th>IWLOC6</th>\n",
       "      <th>STRUCTQ</th>\n",
       "      <th>BUILD</th>\n",
       "      <th>OTBUILD</th>\n",
       "      <th>COMBUILD</th>\n",
       "      <th>CASECOMP</th>\n",
       "      <th>CASEDIF</th>\n",
       "      <th>total_sum</th>\n",
       "      <th>depression_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2763 rows × 318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GENDER  AGE  AGEGRP  DEGREE_RECODE  EDUC  RACE_RECODE  HISPANIC  ETHGRP  \\\n",
       "0          1   62       0              5     3            2       0.0     0.0   \n",
       "1          1   79       2              2     2            2       0.0     0.0   \n",
       "2          0   58       0              4     3            2       0.0     0.0   \n",
       "3          0   79       2              5     3            2       0.0     0.0   \n",
       "4          1   68       1              6     3            2       0.0     0.0   \n",
       "...      ...  ...     ...            ...   ...          ...       ...     ...   \n",
       "2758       1   61       0              1     0            2       1.0     2.0   \n",
       "2759       1   70       1              1     0            4       1.0     2.0   \n",
       "2760       1   70       1              3     2            2       0.0     0.0   \n",
       "2761       0   63       0              4     3            2       0.0     0.0   \n",
       "2762       1   73       1              3     2            2       0.0     0.0   \n",
       "\n",
       "      MILITARY  JAIL  ...  IWLOC5  IWLOC6  STRUCTQ  BUILD  OTBUILD  COMBUILD  \\\n",
       "0          0.0   0.0  ...     0.0     0.0      1.0    3.0      2.0       2.0   \n",
       "1          0.0   0.0  ...     0.0     0.0      1.0    3.0      2.0       3.0   \n",
       "2          0.0   0.0  ...     0.0     0.0      1.0    2.0      2.0       2.0   \n",
       "3          1.0   0.0  ...     0.0     0.0      1.0    3.0      3.0       4.0   \n",
       "4          0.0   0.0  ...     0.0     0.0      1.0    3.0      2.0       3.0   \n",
       "...        ...   ...  ...     ...     ...      ...    ...      ...       ...   \n",
       "2758       0.0   0.0  ...     0.0     0.0      0.0    2.0      0.0       3.0   \n",
       "2759       0.0   0.0  ...     0.0     0.0      1.0    NaN      NaN       NaN   \n",
       "2760       0.0   0.0  ...     0.0     3.0      1.0    2.0      2.0       2.0   \n",
       "2761       0.0   0.0  ...     1.0     0.0      1.0    3.0      3.0       2.0   \n",
       "2762       0.0   0.0  ...     1.0     1.0      1.0    3.0      3.0       2.0   \n",
       "\n",
       "      CASECOMP  CASEDIF  total_sum  depression_category  \n",
       "0         10.0      1.0          2               normal  \n",
       "1         10.0      2.0          3               normal  \n",
       "2         10.0      1.0          2               normal  \n",
       "3          7.0      1.0          4               normal  \n",
       "4         10.0      1.0          3               normal  \n",
       "...        ...      ...        ...                  ...  \n",
       "2758      10.0      2.0         14       moderatesevere  \n",
       "2759      10.0      2.0         11       moderatesevere  \n",
       "2760      10.0      3.0         10       moderatesevere  \n",
       "2761       6.0      3.0         11       moderatesevere  \n",
       "2762       4.0      2.0         17       moderatesevere  \n",
       "\n",
       "[2763 rows x 318 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_initial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba54a81-d4de-4884-99d3-29867eb7ea40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal            1308\n",
       "mild               940\n",
       "moderatesevere     515\n",
       "Name: depression_category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate and Combine\n",
    "le_df_normal = le_initial_df[le_initial_df['depression_category'] == 'normal']\n",
    "le_df_mild = le_initial_df[le_initial_df['depression_category'] == 'mild']\n",
    "le_df_moderatesevere = le_initial_df[le_initial_df['depression_category'] == 'moderatesevere']\n",
    "\n",
    "le_df_depression = pd.concat([le_df_normal, le_df_mild, le_df_moderatesevere], ignore_index = False)\n",
    "\n",
    "le_df_depression['depression_category'] = 'depression'\n",
    "\n",
    "# Check depression category counts\n",
    "dataframes = [le_df_normal, le_df_mild, le_df_moderatesevere]\n",
    "le_initial_df = pd.concat(dataframes, ignore_index=True)\n",
    "label_counts = le_initial_df['depression_category'].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c517213f-f6bb-4298-99ee-3b29f6f7d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some outlier.\n",
    "# threshold = int(0.8 * le_df_normal.shape[1])\n",
    "# le_df_normal = le_df_normal.dropna(thresh = threshold)\n",
    "# threshold = int(0.8 * le_df_depression.shape[1])\n",
    "# le_df_depression = le_df_depression.dropna(thresh = threshold)\n",
    "\n",
    "# Check depression category counts\n",
    "dataframes = [le_df_normal, le_df_mild, le_df_moderatesevere]\n",
    "le_initial_df = pd.concat(dataframes, ignore_index=True)\n",
    "label_counts = le_initial_df['depression_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5bdf52-c645-4a11-920a-579e28db1a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGEGRP</th>\n",
       "      <th>DEGREE_RECODE</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>RACE_RECODE</th>\n",
       "      <th>HISPANIC</th>\n",
       "      <th>ETHGRP</th>\n",
       "      <th>MILITARY</th>\n",
       "      <th>JAIL</th>\n",
       "      <th>...</th>\n",
       "      <th>IWLOC5</th>\n",
       "      <th>IWLOC6</th>\n",
       "      <th>STRUCTQ</th>\n",
       "      <th>BUILD</th>\n",
       "      <th>OTBUILD</th>\n",
       "      <th>COMBUILD</th>\n",
       "      <th>CASECOMP</th>\n",
       "      <th>CASEDIF</th>\n",
       "      <th>total_sum</th>\n",
       "      <th>depression_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>moderatesevere</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2763 rows × 318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GENDER   AGE  AGEGRP  DEGREE_RECODE  EDUC  RACE_RECODE  HISPANIC  \\\n",
       "0        1.0  62.0     0.0            5.0   3.0          2.0       0.0   \n",
       "1        1.0  79.0     2.0            2.0   2.0          2.0       0.0   \n",
       "2        0.0  58.0     0.0            4.0   3.0          2.0       0.0   \n",
       "3        0.0  79.0     2.0            5.0   3.0          2.0       0.0   \n",
       "4        1.0  68.0     1.0            6.0   3.0          2.0       0.0   \n",
       "...      ...   ...     ...            ...   ...          ...       ...   \n",
       "2758     1.0  61.0     0.0            1.0   0.0          2.0       1.0   \n",
       "2759     1.0  70.0     1.0            1.0   0.0          4.0       1.0   \n",
       "2760     1.0  70.0     1.0            3.0   2.0          2.0       0.0   \n",
       "2761     0.0  63.0     0.0            4.0   3.0          2.0       0.0   \n",
       "2762     1.0  73.0     1.0            3.0   2.0          2.0       0.0   \n",
       "\n",
       "      ETHGRP  MILITARY  JAIL  ...  IWLOC5  IWLOC6  STRUCTQ  BUILD  OTBUILD  \\\n",
       "0        0.0       0.0   0.0  ...     0.0     0.0      1.0    3.0      2.0   \n",
       "1        0.0       0.0   0.0  ...     0.0     0.0      1.0    3.0      2.0   \n",
       "2        0.0       0.0   0.0  ...     0.0     0.0      1.0    2.0      2.0   \n",
       "3        0.0       1.0   0.0  ...     0.0     0.0      1.0    3.0      3.0   \n",
       "4        0.0       0.0   0.0  ...     0.0     0.0      1.0    3.0      2.0   \n",
       "...      ...       ...   ...  ...     ...     ...      ...    ...      ...   \n",
       "2758     2.0       0.0   0.0  ...     0.0     0.0      0.0    2.0      0.0   \n",
       "2759     2.0       0.0   0.0  ...     0.0     0.0      1.0    2.0      2.0   \n",
       "2760     0.0       0.0   0.0  ...     0.0     3.0      1.0    2.0      2.0   \n",
       "2761     0.0       0.0   0.0  ...     1.0     0.0      1.0    3.0      3.0   \n",
       "2762     0.0       0.0   0.0  ...     1.0     1.0      1.0    3.0      3.0   \n",
       "\n",
       "      COMBUILD  CASECOMP  CASEDIF  total_sum  depression_category  \n",
       "0          2.0      10.0      1.0        2.0               normal  \n",
       "1          3.0      10.0      2.0        3.0               normal  \n",
       "2          2.0      10.0      1.0        2.0               normal  \n",
       "3          4.0       7.0      1.0        4.0               normal  \n",
       "4          3.0      10.0      1.0        3.0               normal  \n",
       "...        ...       ...      ...        ...                  ...  \n",
       "2758       3.0      10.0      2.0       14.0       moderatesevere  \n",
       "2759       2.0      10.0      2.0       11.0       moderatesevere  \n",
       "2760       2.0      10.0      3.0       10.0       moderatesevere  \n",
       "2761       2.0       6.0      3.0       11.0       moderatesevere  \n",
       "2762       2.0       4.0      2.0       17.0       moderatesevere  \n",
       "\n",
       "[2763 rows x 318 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputation\n",
    "different_le_dfs = [le_df_normal, le_df_mild, le_df_moderatesevere]\n",
    "imputed_le_dfs = []\n",
    "from sklearn.impute import IterativeImputer\n",
    "for le_df in different_le_dfs:\n",
    "    y = le_df['depression_category']\n",
    "    X = le_df.drop('depression_category', axis = 1)\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    imputed_data = imputer.fit_transform(X)\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns = X.columns)\n",
    "\n",
    "    imputed_df['depression_category'] = y.reset_index(drop = True)\n",
    "    imputed_le_dfs.append(imputed_df)\n",
    "\n",
    "concatenated_le_dfs = pd.concat(imputed_le_dfs, ignore_index = True)\n",
    "concatenated_le_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e871a5f-beab-4f90-b8c4-e922ef86d0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AGEGRP</th>\n",
       "      <th>DEGREE_RECODE</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>RACE_RECODE</th>\n",
       "      <th>HISPANIC</th>\n",
       "      <th>ETHGRP</th>\n",
       "      <th>MILITARY</th>\n",
       "      <th>JAIL</th>\n",
       "      <th>...</th>\n",
       "      <th>IWLOC4</th>\n",
       "      <th>IWLOC5</th>\n",
       "      <th>IWLOC6</th>\n",
       "      <th>STRUCTQ</th>\n",
       "      <th>BUILD</th>\n",
       "      <th>OTBUILD</th>\n",
       "      <th>COMBUILD</th>\n",
       "      <th>CASECOMP</th>\n",
       "      <th>CASEDIF</th>\n",
       "      <th>total_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2763 rows × 317 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GENDER   AGE  AGEGRP  DEGREE_RECODE  EDUC  RACE_RECODE  HISPANIC  \\\n",
       "0        1.0  62.0     0.0            5.0   3.0          2.0       0.0   \n",
       "1        1.0  79.0     2.0            2.0   2.0          2.0       0.0   \n",
       "2        0.0  58.0     0.0            4.0   3.0          2.0       0.0   \n",
       "3        0.0  79.0     2.0            5.0   3.0          2.0       0.0   \n",
       "4        1.0  68.0     1.0            6.0   3.0          2.0       0.0   \n",
       "...      ...   ...     ...            ...   ...          ...       ...   \n",
       "2758     1.0  61.0     0.0            1.0   0.0          2.0       1.0   \n",
       "2759     1.0  70.0     1.0            1.0   0.0          4.0       1.0   \n",
       "2760     1.0  70.0     1.0            3.0   2.0          2.0       0.0   \n",
       "2761     0.0  63.0     0.0            4.0   3.0          2.0       0.0   \n",
       "2762     1.0  73.0     1.0            3.0   2.0          2.0       0.0   \n",
       "\n",
       "      ETHGRP  MILITARY  JAIL  ...  IWLOC4  IWLOC5  IWLOC6  STRUCTQ  BUILD  \\\n",
       "0        0.0       0.0   0.0  ...     0.0     0.0     0.0      1.0    3.0   \n",
       "1        0.0       0.0   0.0  ...     0.0     0.0     0.0      1.0    3.0   \n",
       "2        0.0       0.0   0.0  ...     0.0     0.0     0.0      1.0    2.0   \n",
       "3        0.0       1.0   0.0  ...     0.0     0.0     0.0      1.0    3.0   \n",
       "4        0.0       0.0   0.0  ...     0.0     0.0     0.0      1.0    3.0   \n",
       "...      ...       ...   ...  ...     ...     ...     ...      ...    ...   \n",
       "2758     2.0       0.0   0.0  ...     0.0     0.0     0.0      0.0    2.0   \n",
       "2759     2.0       0.0   0.0  ...     0.0     0.0     0.0      1.0    2.0   \n",
       "2760     0.0       0.0   0.0  ...     1.0     0.0     3.0      1.0    2.0   \n",
       "2761     0.0       0.0   0.0  ...     1.0     1.0     0.0      1.0    3.0   \n",
       "2762     0.0       0.0   0.0  ...     2.0     1.0     1.0      1.0    3.0   \n",
       "\n",
       "      OTBUILD  COMBUILD  CASECOMP  CASEDIF  total_sum  \n",
       "0         2.0       2.0      10.0      1.0        2.0  \n",
       "1         2.0       3.0      10.0      2.0        3.0  \n",
       "2         2.0       2.0      10.0      1.0        2.0  \n",
       "3         3.0       4.0       7.0      1.0        4.0  \n",
       "4         2.0       3.0      10.0      1.0        3.0  \n",
       "...       ...       ...       ...      ...        ...  \n",
       "2758      0.0       3.0      10.0      2.0       14.0  \n",
       "2759      2.0       2.0      10.0      2.0       11.0  \n",
       "2760      2.0       2.0      10.0      3.0       10.0  \n",
       "2761      3.0       2.0       6.0      3.0       11.0  \n",
       "2762      3.0       2.0       4.0      2.0       17.0  \n",
       "\n",
       "[2763 rows x 317 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full label encode depression category\n",
    "fully_LE_concatenated_le_dfs = concatenated_le_dfs.copy()\n",
    "fully_LE_concatenated_le_dfs['depression_category'] = label_encoder.fit_transform(fully_LE_concatenated_le_dfs['depression_category'])\n",
    "\n",
    "# The dataset after category connect, imputation, and label encoding\n",
    "splitted_dataset = fully_LE_concatenated_le_dfs.copy()\n",
    "splitted_dataset = splitted_dataset.drop('depression_category', axis = 1)\n",
    "splitted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e602f4d-efb2-4ac9-99bb-c6fbeca791cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68563dc9-38c9-4ea5-8c08-9de904f58f43",
   "metadata": {},
   "source": [
    "### Regression Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e339a41-e484-4a11-84f0-770cbaacb09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for Random State: 0\n",
      "Number of training labels after outlier removal: 1842\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 1\n",
      "Number of training labels after outlier removal: 1838\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 2\n",
      "Number of training labels after outlier removal: 1859\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 3\n",
      "Number of training labels after outlier removal: 1847\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 4\n",
      "Number of training labels after outlier removal: 1846\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 5\n",
      "Number of training labels after outlier removal: 1833\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 6\n",
      "Number of training labels after outlier removal: 1840\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 7\n",
      "Number of training labels after outlier removal: 1849\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 8\n",
      "Number of training labels after outlier removal: 1849\n",
      "Number of test labels: 829\n",
      "Processing for Random State: 9\n",
      "Number of training labels after outlier removal: 1837\n",
      "Number of test labels: 829\n"
     ]
    }
   ],
   "source": [
    "# Optimized parameters\n",
    "regressors = {\n",
    "    'CBR': CatBoostRegressor(verbose=0, iterations=2000, learning_rate=0.01, depth=5),\n",
    "    'XGBR': XGBRegressor(learning_rate=0.04403027347366962, max_depth=3, n_estimators=238),\n",
    "    'LGBMR': LGBMRegressor(learning_rate=0.02904035023286438, num_leaves=20, n_estimators=170),\n",
    "    'GBR': GradientBoostingRegressor(learning_rate=0.03, max_depth=2, n_estimators=700),\n",
    "    'RFR': RandomForestRegressor(max_depth=12, max_features=1.0, n_estimators=300),\n",
    "    'ETR': ExtraTreesRegressor(max_depth=12, max_features=1.0, n_estimators=64),\n",
    "    'ABR': AdaBoostRegressor(learning_rate=0.29915504677867777, n_estimators=92)\n",
    "}\n",
    "\n",
    "# Default parameters\n",
    "# regressors = {\n",
    "#     'CBR': CatBoostRegressor(verbose=0),\n",
    "#     'XGBR': XGBRegressor(),\n",
    "#     'LGBMR': LGBMRegressor(),\n",
    "#     'GBR': GradientBoostingRegressor(),\n",
    "#     'RFR': RandomForestRegressor(),\n",
    "#     'ETR': ExtraTreesRegressor(),\n",
    "#     'ABR': AdaBoostRegressor()\n",
    "# }\n",
    "\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('cbr', regressors['CBR']),\n",
    "    ('xgbr', regressors['XGBR']),\n",
    "    ('gbr', regressors['GBR']),\n",
    "    ('abr', regressors['ABR'])\n",
    "])\n",
    "\n",
    "regressors['Voting'] = voting_regressor\n",
    "\n",
    "metric_sums = {name: {'rmse': 0, 'mae': 0, 'r2': 0} for name in regressors.keys()}\n",
    "metric_stds = {name: {'rmse': 0, 'mae': 0, 'r2': 0} for name in regressors.keys()}\n",
    "rmse_scores = {name: [] for name in regressors.keys()}\n",
    "mae_scores = {name: [] for name in regressors.keys()}\n",
    "r2_scores = {name: [] for name in regressors.keys()}\n",
    "\n",
    "for random_state in range(10):\n",
    "    print(f'Processing for Random State: {random_state}')\n",
    "\n",
    "    X = splitted_dataset.drop('total_sum', axis=1)\n",
    "    y = splitted_dataset['total_sum']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "    lof = LocalOutlierFactor()\n",
    "    yhat = lof.fit_predict(X_train)\n",
    "\n",
    "    mask = yhat != -1\n",
    "    X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "    original_columns = X.columns.tolist()\n",
    "\n",
    "    print(f\"Number of training labels after outlier removal: {len(y_train)}\")\n",
    "    print(f\"Number of test labels: {len(y_test)}\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train, columns=original_columns)\n",
    "    X_test = pd.DataFrame(X_test, columns=original_columns)\n",
    "\n",
    "    # Feature selection using XGBRegressor\n",
    "    xgb = XGBRegressor(random_state=random_state)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    selector = SelectFromModel(xgb, prefit=True)\n",
    "\n",
    "    importance = np.abs(xgb.feature_importances_)\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    important_features = [original_columns[i] for i in indices[:50]]\n",
    "\n",
    "    for reg_name, reg in regressors.items():\n",
    "        selected_features = important_features\n",
    "        \n",
    "        X_train_fi = pd.DataFrame(X_train, columns=original_columns)[selected_features]\n",
    "        X_test_fi = pd.DataFrame(X_test, columns=original_columns)[selected_features]\n",
    "\n",
    "        reg.fit(X_train_fi, y_train)\n",
    "        y_pred = reg.predict(X_test_fi)\n",
    "\n",
    "        y_pred = np.round(y_pred)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        metric_sums[reg_name]['rmse'] += rmse\n",
    "        metric_sums[reg_name]['mae'] += mae\n",
    "        metric_sums[reg_name]['r2'] += r2\n",
    "        rmse_scores[reg_name].append(rmse)\n",
    "        mae_scores[reg_name].append(mae)\n",
    "        r2_scores[reg_name].append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0929bf2c-fd41-4e0e-8070-4a6317f3efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor: CBR\n",
      "Average RMSE: 2.33212820158642 ± 0.055097288466959646\n",
      "Average MAE: 1.8145958986731003 ± 0.056023669163409784\n",
      "Average R2: 0.7425974040934744 ± 0.008382037411545003\n",
      "------\n",
      "Regressor: XGBR\n",
      "Average RMSE: 2.390647518591595 ± 0.06772970382837729\n",
      "Average MAE: 1.860193003618818 ± 0.06221011148884051\n",
      "Average R2: 0.7293457710895285 ± 0.01435860604641583\n",
      "------\n",
      "Regressor: LGBMR\n",
      "Average RMSE: 2.3751746454600484 ± 0.06800748162148057\n",
      "Average MAE: 1.8521109770808202 ± 0.06807739278882169\n",
      "Average R2: 0.7328791730997409 ± 0.013508317451016535\n",
      "------\n",
      "Regressor: GBR\n",
      "Average RMSE: 2.376531051190981 ± 0.05802369320874543\n",
      "Average MAE: 1.8563329312424606 ± 0.054385239419334125\n",
      "Average R2: 0.7326307111215877 ± 0.010908685476682169\n",
      "------\n",
      "Regressor: RFR\n",
      "Average RMSE: 2.4845736390091337 ± 0.06676197760203786\n",
      "Average MAE: 1.9213510253317245 ± 0.056212303980699686\n",
      "Average R2: 0.7078724320067347 ± 0.010168136014613706\n",
      "------\n",
      "Regressor: ETR\n",
      "Average RMSE: 2.573527627227243 ± 0.06011802348157688\n",
      "Average MAE: 1.9907117008443909 ± 0.06274948532301211\n",
      "Average R2: 0.6864188517123799 ± 0.013449644910168078\n",
      "------\n",
      "Regressor: ABR\n",
      "Average RMSE: 2.6852857383852697 ± 0.06841631917973365\n",
      "Average MAE: 2.179734620024125 ± 0.05749252721322697\n",
      "Average R2: 0.6585921484860847 ± 0.015370997472186695\n",
      "------\n",
      "Regressor: Voting\n",
      "Average RMSE: 2.371930267773409 ± 0.06214793429498248\n",
      "Average MAE: 1.8757539203860074 ± 0.05758104150464501\n",
      "Average R2: 0.7336512016529826 ± 0.01170151252673983\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average metrics and their standard deviations\n",
    "for reg_name in regressors.keys():\n",
    "    avg_rmse = metric_sums[reg_name]['rmse'] / 10\n",
    "    avg_mae = metric_sums[reg_name]['mae'] / 10\n",
    "    avg_r2 = metric_sums[reg_name]['r2'] / 10\n",
    "    std_rmse = np.std(rmse_scores[reg_name])\n",
    "    std_mae = np.std(mae_scores[reg_name])\n",
    "    std_r2 = np.std(r2_scores[reg_name])\n",
    "    \n",
    "    print(f\"Regressor: {reg_name}\")\n",
    "    print(f\"Average RMSE: {avg_rmse} ± {std_rmse}\")\n",
    "    print(f\"Average MAE: {avg_mae} ± {std_mae}\")\n",
    "    print(f\"Average R2: {avg_r2} ± {std_r2}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f62248-70b4-479b-b782-3aec714111f9",
   "metadata": {},
   "source": [
    "### Shap n FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ce4639-53d9-4dcd-a49c-80555326f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation code to make CD diagram from older version of Orange\n",
    "def compute_CD(avranks, n, alpha=\"0.05\", test=\"nemenyi\"):\n",
    "    \"\"\"\n",
    "    Returns critical difference for Nemenyi or Bonferroni-Dunn test\n",
    "    according to given alpha (either alpha=\"0.05\" or alpha=\"0.1\") for average\n",
    "    ranks and number of tested datasets N. Test can be either \"nemenyi\" for\n",
    "    for Nemenyi two tailed test or \"bonferroni-dunn\" for Bonferroni-Dunn test.\n",
    "\n",
    "    This function is deprecated and will be removed in Orange 3.34.\n",
    "    \"\"\"\n",
    "    k = len(avranks)\n",
    "    d = {(\"nemenyi\", \"0.05\"): [0, 0, 1.959964, 2.343701, 2.569032, 2.727774,\n",
    "                               2.849705, 2.94832, 3.030879, 3.101730, 3.163684,\n",
    "                               3.218654, 3.268004, 3.312739, 3.353618, 3.39123,\n",
    "                               3.426041, 3.458425, 3.488685, 3.517073,\n",
    "                               3.543799],\n",
    "         (\"nemenyi\", \"0.1\"): [0, 0, 1.644854, 2.052293, 2.291341, 2.459516,\n",
    "                              2.588521, 2.692732, 2.779884, 2.854606, 2.919889,\n",
    "                              2.977768, 3.029694, 3.076733, 3.119693, 3.159199,\n",
    "                              3.195743, 3.229723, 3.261461, 3.291224, 3.319233],\n",
    "         (\"bonferroni-dunn\", \"0.05\"): [0, 0, 1.960, 2.241, 2.394, 2.498, 2.576,\n",
    "                                       2.638, 2.690, 2.724, 2.773],\n",
    "         (\"bonferroni-dunn\", \"0.1\"): [0, 0, 1.645, 1.960, 2.128, 2.241, 2.326,\n",
    "                                      2.394, 2.450, 2.498, 2.539]}\n",
    "    q = d[(test, alpha)]\n",
    "    cd = q[k] * (k * (k + 1) / (6.0 * n)) ** 0.5\n",
    "    return cd\n",
    "\n",
    "\n",
    "def graph_ranks(avranks, names, cd=None, cdmethod=None, lowv=None, highv=None,\n",
    "                width=6, textspace=1, reverse=False, filename=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Draws a CD graph, which is used to display  the differences in methods'\n",
    "    performance. See Janez Demsar, Statistical Comparisons of Classifiers over\n",
    "    Multiple Data Sets, 7(Jan):1--30, 2006.\n",
    "\n",
    "    Needs matplotlib to work.\n",
    "\n",
    "    The image is ploted on `plt` imported using\n",
    "    `import matplotlib.pyplot as plt`.\n",
    "\n",
    "    This function is deprecated and will be removed in Orange 3.34.\n",
    "\n",
    "    Args:\n",
    "        avranks (list of float): average ranks of methods.\n",
    "        names (list of str): names of methods.\n",
    "        cd (float): Critical difference used for statistically significance of\n",
    "            difference between methods.\n",
    "        cdmethod (int, optional): the method that is compared with other methods\n",
    "            If omitted, show pairwise comparison of methods\n",
    "        lowv (int, optional): the lowest shown rank\n",
    "        highv (int, optional): the highest shown rank\n",
    "        width (int, optional): default width in inches (default: 6)\n",
    "        textspace (int, optional): space on figure sides (in inches) for the\n",
    "            method names (default: 1)\n",
    "        reverse (bool, optional):  if set to `True`, the lowest rank is on the\n",
    "            right (default: `False`)\n",
    "        filename (str, optional): output file name (with extension). If not\n",
    "            given, the function does not write a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Function graph_ranks requires matplotlib.\")\n",
    "\n",
    "    width = float(width)\n",
    "    textspace = float(textspace)\n",
    "\n",
    "    def nth(l, n):\n",
    "        \"\"\"\n",
    "        Returns only nth elemnt in a list.\n",
    "        \"\"\"\n",
    "        n = lloc(l, n)\n",
    "        return [a[n] for a in l]\n",
    "\n",
    "    def lloc(l, n):\n",
    "        \"\"\"\n",
    "        List location in list of list structure.\n",
    "        Enable the use of negative locations:\n",
    "        -1 is the last element, -2 second last...\n",
    "        \"\"\"\n",
    "        if n < 0:\n",
    "            return len(l[0]) + n\n",
    "        else:\n",
    "            return n\n",
    "\n",
    "    def mxrange(lr):\n",
    "        \"\"\"\n",
    "        Multiple xranges. Can be used to traverse matrices.\n",
    "        This function is very slow due to unknown number of\n",
    "        parameters.\n",
    "\n",
    "        >>> mxrange([3,5])\n",
    "        [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n",
    "\n",
    "        >>> mxrange([[3,5,1],[9,0,-3]])\n",
    "        [(3, 9), (3, 6), (3, 3), (4, 9), (4, 6), (4, 3)]\n",
    "\n",
    "        \"\"\"\n",
    "        if not len(lr):\n",
    "            yield ()\n",
    "        else:\n",
    "            # it can work with single numbers\n",
    "            index = lr[0]\n",
    "            if isinstance(index, int):\n",
    "                index = [index]\n",
    "            for a in range(*index):\n",
    "                for b in mxrange(lr[1:]):\n",
    "                    yield tuple([a] + list(b))\n",
    "\n",
    "    def print_figure(fig, *args, **kwargs):\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        canvas.print_figure(*args, **kwargs)\n",
    "\n",
    "    sums = avranks\n",
    "\n",
    "    tempsort = sorted([(a, i) for i, a in enumerate(sums)], reverse=reverse)\n",
    "    ssums = nth(tempsort, 0)\n",
    "    sortidx = nth(tempsort, 1)\n",
    "    nnames = [names[x] for x in sortidx]\n",
    "\n",
    "    if lowv is None:\n",
    "        lowv = min(1, int(math.floor(min(ssums))))\n",
    "    if highv is None:\n",
    "        highv = max(len(avranks), int(math.ceil(max(ssums))))\n",
    "\n",
    "    cline = 0.4\n",
    "\n",
    "    k = len(sums)\n",
    "\n",
    "    lines = None\n",
    "\n",
    "    linesblank = 0\n",
    "    scalewidth = width - 2 * textspace\n",
    "\n",
    "    def rankpos(rank):\n",
    "        if not reverse:\n",
    "            a = rank - lowv\n",
    "        else:\n",
    "            a = highv - rank\n",
    "        return textspace + scalewidth / (highv - lowv) * a\n",
    "\n",
    "    distanceh = 0.25\n",
    "\n",
    "    if cd and cdmethod is None:\n",
    "        # get pairs of non significant methods\n",
    "\n",
    "        def get_lines(sums, hsd):\n",
    "            # get all pairs\n",
    "            lsums = len(sums)\n",
    "            allpairs = [(i, j) for i, j in mxrange([[lsums], [lsums]]) if j > i]\n",
    "            # remove not significant\n",
    "            notSig = [(i, j) for i, j in allpairs\n",
    "                      if abs(sums[i] - sums[j]) <= hsd]\n",
    "            # keep only longest\n",
    "\n",
    "            def no_longer(ij_tuple, notSig):\n",
    "                i, j = ij_tuple\n",
    "                for i1, j1 in notSig:\n",
    "                    if (i1 <= i and j1 > j) or (i1 < i and j1 >= j):\n",
    "                        return False\n",
    "                return True\n",
    "\n",
    "            longest = [(i, j) for i, j in notSig if no_longer((i, j), notSig)]\n",
    "\n",
    "            return longest\n",
    "\n",
    "        lines = get_lines(ssums, cd)\n",
    "        linesblank = 0.2 + 0.2 + (len(lines) - 1) * 0.1\n",
    "\n",
    "        # add scale\n",
    "        distanceh = 0.25\n",
    "        cline += distanceh\n",
    "\n",
    "    # calculate height needed height of an image\n",
    "    minnotsignificant = max(2 * 0.2, linesblank)\n",
    "    height = cline + ((k + 1) / 2) * 0.2 + minnotsignificant\n",
    "\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    fig.set_facecolor('white')\n",
    "    ax = fig.add_axes([0, 0, 1, 1])  # reverse y axis\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    hf = 1. / height  # height factor\n",
    "    wf = 1. / width\n",
    "\n",
    "    def hfl(l):\n",
    "        return [a * hf for a in l]\n",
    "\n",
    "    def wfl(l):\n",
    "        return [a * wf for a in l]\n",
    "\n",
    "\n",
    "    # Upper left corner is (0,0).\n",
    "    ax.plot([0, 1], [0, 1], c=\"w\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(1, 0)\n",
    "\n",
    "    def line(l, color='k', **kwargs):\n",
    "        \"\"\"\n",
    "        Input is a list of pairs of points.\n",
    "        \"\"\"\n",
    "        ax.plot(wfl(nth(l, 0)), hfl(nth(l, 1)), color=color, **kwargs)\n",
    "\n",
    "    def text(x, y, s, *args, **kwargs):\n",
    "        ax.text(wf * x, hf * y, s, fontsize = 14, *args, **kwargs)\n",
    "\n",
    "    line([(textspace, cline), (width - textspace, cline)], linewidth=0.7)\n",
    "\n",
    "    bigtick = 0.1\n",
    "    smalltick = 0.05\n",
    "\n",
    "    tick = None\n",
    "    for a in list(np.arange(lowv, highv, 0.5)) + [highv]:\n",
    "        tick = smalltick\n",
    "        if a == int(a):\n",
    "            tick = bigtick\n",
    "        line([(rankpos(a), cline - tick / 2),\n",
    "              (rankpos(a), cline)],\n",
    "             linewidth=0.7)\n",
    "\n",
    "    for a in range(lowv, highv + 1):\n",
    "        text(rankpos(a), cline - tick / 2 - 0.05, str(a),\n",
    "             ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    k = len(ssums)\n",
    "\n",
    "    for i in range(math.ceil(k / 2)):\n",
    "        chei = cline + minnotsignificant + i * 0.2\n",
    "        line([(rankpos(ssums[i]), cline),\n",
    "              (rankpos(ssums[i]), chei),\n",
    "              (textspace - 0.1, chei)],\n",
    "             linewidth=0.7)\n",
    "        text(textspace - 0.2, chei, nnames[i], ha=\"right\", va=\"center\")\n",
    "\n",
    "    for i in range(math.ceil(k / 2), k):\n",
    "        chei = cline + minnotsignificant + (k - i - 1) * 0.2\n",
    "        line([(rankpos(ssums[i]), cline),\n",
    "              (rankpos(ssums[i]), chei),\n",
    "              (textspace + scalewidth + 0.1, chei)],\n",
    "             linewidth=0.7)\n",
    "        text(textspace + scalewidth + 0.2, chei, nnames[i],\n",
    "             ha=\"left\", va=\"center\")\n",
    "\n",
    "    if cd and cdmethod is None:\n",
    "        # upper scale\n",
    "        if not reverse:\n",
    "            begin, end = rankpos(lowv), rankpos(lowv + cd)\n",
    "        else:\n",
    "            begin, end = rankpos(highv), rankpos(highv - cd)\n",
    "\n",
    "        line([(begin, distanceh), (end, distanceh)], linewidth=0.7)\n",
    "        line([(begin, distanceh + bigtick / 2),\n",
    "              (begin, distanceh - bigtick / 2)],\n",
    "             linewidth=0.7)\n",
    "        line([(end, distanceh + bigtick / 2),\n",
    "              (end, distanceh - bigtick / 2)],\n",
    "             linewidth=0.7)\n",
    "        text((begin + end) / 2, distanceh - 0.05, \"CD\",\n",
    "             ha=\"center\", va=\"bottom\")\n",
    "\n",
    "        # no-significance lines\n",
    "        def draw_lines(lines, side=0.05, height=0.1):\n",
    "            start = cline + 0.2\n",
    "            for l, r in lines:\n",
    "                line([(rankpos(ssums[l]) - side, start),\n",
    "                      (rankpos(ssums[r]) + side, start)],\n",
    "                     linewidth=2.5)\n",
    "                start += height\n",
    "\n",
    "        draw_lines(lines)\n",
    "\n",
    "    elif cd:\n",
    "        begin = rankpos(avranks[cdmethod] - cd)\n",
    "        end = rankpos(avranks[cdmethod] + cd)\n",
    "        line([(begin, cline), (end, cline)],\n",
    "             linewidth=2.5)\n",
    "        line([(begin, cline + bigtick / 2),\n",
    "              (begin, cline - bigtick / 2)],\n",
    "             linewidth=2.5)\n",
    "        line([(end, cline + bigtick / 2),\n",
    "              (end, cline - bigtick / 2)],\n",
    "             linewidth=2.5)\n",
    "\n",
    "    if filename:\n",
    "        print_figure(fig, filename, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeeb1424-63f5-4338-bdc9-5e36e8e4ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman Test Statistic: 62.0, p-value: 6.011355572699598e-11\n",
      "Critical Difference: 6.928203230275509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x195 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FN\n",
    "df = pd.DataFrame(rmse_scores)\n",
    "df\n",
    "\n",
    "scores = [df[col].values for col in df.columns]\n",
    "\n",
    "stat, p = friedmanchisquare(*scores)\n",
    "print(f'Friedman Test Statistic: {stat}, p-value: {p}')\n",
    "\n",
    "ranks = df.rank(axis=1, method='average')\n",
    "average_ranks = ranks.mean().values\n",
    "\n",
    "n_datasets = df.shape[0]\n",
    "alpha = 0.05\n",
    "\n",
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "cd = np.sqrt((len(df.columns) * (len(df.columns) + 1)) / (6 * n_datasets)) * np.sqrt(2 / alpha)\n",
    "print(f'Critical Difference: {cd}')\n",
    "\n",
    "classifiers = [f\"{clf} ({rank:.2f})\" for clf, rank in zip(df.columns, average_ranks)]\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "graph_ranks(average_ranks, classifiers, cd=cd, width=6, textspace=1)\n",
    "plt.xlabel('Classifiers')\n",
    "\n",
    "plt.text(0.5, 1.19, f'Friedman-Nemenyi: {stat:.3f}', horizontalalignment='center', transform=plt.gca().transAxes, fontsize=16)\n",
    "plt.text(0.5, 1.10, f'CD: {cd:.3f}', horizontalalignment='center', transform=plt.gca().transAxes, fontsize=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88fdb63-9223-48f5-b4c3-78c5ab76938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='shap_summary_plot.svg' target='_blank'>shap_summary_plot.svg</a><br>"
      ],
      "text/plain": [
       "/home/researchsrv1/clement/DES4DepressionTest/code/shap_summary_plot.svg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array position with low target value: 482, Target Value: 0.0\n",
      "Array position with high target value: 70, Target Value: 27.0\n"
     ]
    }
   ],
   "source": [
    "# SHAP\n",
    "if not hasattr(np, 'bool'):\n",
    "    np.bool = bool\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "cbr = CatBoostRegressor(verbose=0, iterations=2000, learning_rate=0.01, depth=5)\n",
    "\n",
    "X = splitted_dataset.drop('total_sum', axis=1)\n",
    "y = splitted_dataset['total_sum']\n",
    "random_state = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "original_columns = X.columns.tolist()\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=original_columns)\n",
    "X_test = pd.DataFrame(X_test, columns=original_columns)\n",
    "\n",
    "xgb = XGBRegressor(random_state=random_state)\n",
    "xgb.fit(X_train, y_train)\n",
    "selector = SelectFromModel(xgb, prefit=True)\n",
    "\n",
    "importance = np.abs(xgb.feature_importances_)\n",
    "indices = np.argsort(importance)[::-1]\n",
    "important_features = [original_columns[i] for i in indices[:50]]\n",
    "\n",
    "X_train_fi = X_train[important_features]\n",
    "X_test_fi = X_test[important_features]\n",
    "\n",
    "cbr.fit(X_train_fi, y_train)\n",
    "\n",
    "# Compute SHAP values using shap.Explainer\n",
    "explainer = shap.TreeExplainer(cbr)\n",
    "shap_values = explainer.shap_values(X_train_fi)\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_train_fi, plot_type=\"bar\", show=False)\n",
    "plt.savefig(\"shap_summary_plot.svg\", format='svg')\n",
    "plt.close()\n",
    "display(FileLink(\"shap_summary_plot.svg\"))\n",
    "\n",
    "sorted_indices = np.argsort(y_train.values)\n",
    "low_value_index = sorted_indices[0]\n",
    "high_value_index = sorted_indices[-1]\n",
    "\n",
    "print(f\"Array position with low target value: {low_value_index}, Target Value: {y_train.values[low_value_index]}\")\n",
    "print(f\"Array position with high target value: {high_value_index}, Target Value: {y_train.values[high_value_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6cce3ab-2480-4e87-a4ad-36054e12553a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='waterfall_plot_instance_0.svg' target='_blank'>waterfall_plot_instance_0.svg</a><br>"
      ],
      "text/plain": [
       "/home/researchsrv1/clement/DES4DepressionTest/code/waterfall_plot_instance_0.svg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='waterfall_plot_instance_1.svg' target='_blank'>waterfall_plot_instance_1.svg</a><br>"
      ],
      "text/plain": [
       "/home/researchsrv1/clement/DES4DepressionTest/code/waterfall_plot_instance_1.svg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pour la cohérence des indices\n",
    "shap_values_array = shap_values  # np.ndarray, shape = (n_samples, n_features)\n",
    "\n",
    "# On reconstruit un `shap.Explanation` pour un exemple donné\n",
    "def plot_shap_waterfall(instance_index, filename):\n",
    "    explanation = shap.Explanation(\n",
    "        values=shap_values_array[instance_index],\n",
    "        base_values=cbr.predict(X_train_fi.iloc[[instance_index]])[0],\n",
    "        data=X_train_fi.iloc[instance_index],\n",
    "        feature_names=X_train_fi.columns.tolist()\n",
    "    )\n",
    "\n",
    "    shap.plots.waterfall(explanation, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, format='svg')\n",
    "    plt.close()\n",
    "\n",
    "plot_shap_waterfall(482, \"waterfall_plot_instance_0.svg\")\n",
    "plot_shap_waterfall(70, \"waterfall_plot_instance_1.svg\")\n",
    "\n",
    "display(FileLink(\"waterfall_plot_instance_0.svg\"))\n",
    "display(FileLink(\"waterfall_plot_instance_1.svg\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f6725-ac13-444e-8d6e-fe3cc7a63b95",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "213e4d7c-5719-47bc-831d-46809d65fb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for Random State: 5\n",
      "Number of training labels after outlier removal: 1833\n",
      "Number of test labels: 829\n",
      "Optimizing CBR...\n",
      "Best parameters for CBR: {'iterations': 281, 'learning_rate': 0.1, 'depth': 4, 'loss_function': 'RMSE', 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "if not hasattr(np, 'int'):\n",
    "    np.int = int\n",
    "\n",
    "# Models\n",
    "regressors = {\n",
    "    'CBR': CatBoostRegressor(verbose=0),\n",
    "    #'XGBR': XGBRegressor(),\n",
    "    #'LGBMR': LGBMRegressor(),\n",
    "    # 'GBR': GradientBoostingRegressor(),\n",
    "    #'RFR': RandomForestRegressor(),\n",
    "    # 'ETR': ExtraTreesRegressor(),\n",
    "    # 'ABR': AdaBoostRegressor()\n",
    "}\n",
    "\n",
    "# Define parameter grids for the models\n",
    "param_grids = {\n",
    "    'CBR': {\n",
    "        'iterations': Integer(100, 500),\n",
    "        'learning_rate': Real(0.01, 0.1),\n",
    "        'depth': Integer(3, 10),\n",
    "    },\n",
    "    # 'GBR': {\n",
    "    #     'n_estimators': Integer(50, 300),\n",
    "    #     'learning_rate': Real(0.01, 0.1),\n",
    "    #     'max_depth': Integer(3, 10)\n",
    "    # },\n",
    "    # 'RFR': {\n",
    "    #     'n_estimators': Integer(50, 300),\n",
    "    #     'max_depth': Integer(3, 20)\n",
    "    # },\n",
    "    # 'XGBR': {\n",
    "    #     'n_estimators': Integer(50, 300),\n",
    "    #     'learning_rate': Real(0.01, 0.1),\n",
    "    #     'max_depth': Integer(3, 10),\n",
    "    # },\n",
    "    # 'LGBMR': {\n",
    "    #     'n_estimators': Integer(50, 300),\n",
    "    #     'learning_rate': Real(0.01, 0.1),\n",
    "    #     'num_leaves': Integer(20, 50),\n",
    "    # },\n",
    "    # 'ETR': {\n",
    "    #     'n_estimators': Integer(50, 300),\n",
    "    #     'max_depth': Integer(3, 20)\n",
    "    # },\n",
    "#     'ABR': {\n",
    "#         'n_estimators': Integer(50, 300),\n",
    "#         'learning_rate': Real(0.01, 1.0)\n",
    "#     }\n",
    "}\n",
    "\n",
    "# Function to perform hyperparameter tuning\n",
    "def hyperparameter_tuning(model, param_grid, X_train, y_train):\n",
    "    bayes_search = BayesSearchCV(model, search_spaces=param_grid, n_iter=50, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=0)\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    return bayes_search.best_estimator_\n",
    "\n",
    "metric_sums = {name: {'rmse': 0, 'mae': 0, 'r2': 0} for name in regressors.keys()}\n",
    "metric_stds = {name: {'rmse': 0, 'mae': 0, 'r2': 0} for name in regressors.keys()}\n",
    "rmse_scores = {name: [] for name in regressors.keys()}\n",
    "mae_scores = {name: [] for name in regressors.keys()}\n",
    "r2_scores = {name: [] for name in regressors.keys()}\n",
    "\n",
    "random_state = 5\n",
    "print(f'Processing for Random State: {random_state}')\n",
    "\n",
    "X = splitted_dataset.drop('total_sum', axis=1)\n",
    "y = splitted_dataset['total_sum']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "original_columns = X.columns.tolist()\n",
    "\n",
    "print(f\"Number of training labels after outlier removal: {len(y_train)}\")\n",
    "print(f\"Number of test labels: {len(y_test)}\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=original_columns)\n",
    "X_test = pd.DataFrame(X_test, columns=original_columns)\n",
    "\n",
    "xgb = XGBRegressor(random_state=random_state)\n",
    "xgb.fit(X_train, y_train)\n",
    "selector = SelectFromModel(xgb, prefit=True)\n",
    "\n",
    "importance = np.abs(xgb.feature_importances_)\n",
    "indices = np.argsort(importance)[::-1]\n",
    "important_features = [original_columns[i] for i in indices[:50]]\n",
    "\n",
    "X_train = X_train[important_features]\n",
    "X_test = X_test[important_features]\n",
    "\n",
    "best_models = {}\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    if model_name == 'GBR':\n",
    "        model = GradientBoostingRegressor()\n",
    "    elif model_name == 'RFR':\n",
    "        model = RandomForestRegressor()\n",
    "    elif model_name == 'XGBR':\n",
    "        model = XGBRegressor()\n",
    "    elif model_name == 'LGBMR':\n",
    "        model = LGBMRegressor()\n",
    "    elif model_name == 'ETR':\n",
    "        model = ExtraTreesRegressor()\n",
    "    elif model_name == 'ABR':\n",
    "        model = AdaBoostRegressor()\n",
    "    elif model_name == 'CBR':\n",
    "        model = CatBoostRegressor(verbose=0)\n",
    "    \n",
    "    print(f\"Optimizing {model_name}...\")\n",
    "    best_model = hyperparameter_tuning(model, param_grid, X_train, y_train)\n",
    "    best_models[model_name] = best_model\n",
    "    print(f\"Best parameters for {model_name}: {best_model.get_params()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "des4clem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
